1) Motivation & Problem

BÃ i toÃ¡n. PhÃ¢n loáº¡i vá»›i quyá»n tá»« chá»‘i (abstention) trÃªn dá»¯ liá»‡u long-tail. Chia cÃ¡c lá»›p vÃ o 
ğ¾
K nhÃ³m 
ğº
1
,
â€¦
,
ğº
ğ¾
G
1
	â€‹

,â€¦,G
K
	â€‹

 (vÃ­ dá»¥ head/tail). Vá»›i cáº·p bá»™ quy táº¯c 
(
â„
,
ğ‘Ÿ
)
(h,r) (classifier 
â„
:
ğ‘‹
â†’
ğ‘Œ
h:Xâ†’Y, rejector 
ğ‘Ÿ
:
ğ‘‹
â†’
{
0
,
1
}
r:Xâ†’{0,1} vá»›i 
ğ‘Ÿ
=
1
r=1 lÃ  reject), balanced selective risk lÃ 

	
ğ‘…
bal
rej
(
â„
,
ğ‘Ÿ
)
â€…â€Š
=
â€…â€Š
1
ğ¾
âˆ‘
ğ‘˜
=
1
ğ¾
Pr
â¡
â€‰â£
(
ğ‘¦
â‰ 
â„
(
ğ‘¥
)
âˆ£
ğ‘Ÿ
(
ğ‘¥
)
=
0
,
â€…â€Š
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
â€…â€Š
+
â€…â€Š
ğ‘
â€‰
Pr
â¡
(
ğ‘Ÿ
(
ğ‘¥
)
=
1
)
.
		
(1)
R
bal
rej
	â€‹

(h,r)=
K
1
	â€‹

k=1
âˆ‘
K
	â€‹

Pr(y
î€ 
=h(x)âˆ£r(x)=0,yâˆˆG
k
	â€‹

)+cPr(r(x)=1).
(1)

KhÃ³ khÄƒn. (i) Dá»¯ liá»‡u lá»‡ch lÃ m posterior Æ°á»›c lÆ°á»£ng á»Ÿ tail kÃ©m tin cáº­y; (ii) rejector â€œchuáº©nâ€ kiá»ƒu Chow (ngÆ°á»¡ng 
1
âˆ’
ğ‘
1âˆ’c) thiÃªn vá»‹ head; (iii) náº¿u thiáº¿t káº¿ gating theo máº«u Ä‘á»ƒ trá»™n nhiá»u mÃ´ hÃ¬nh (ensemble) thÃ¬ ráº¥t dá»… overfit tail.

Ã tÆ°á»Ÿng PB-GSE.

DÃ¹ng ensemble 
ğ‘“
1
,
â€¦
,
ğ‘“
ğ‘€
f
1
	â€‹

,â€¦,f
M
	â€‹

 (Ä‘Ã£ huáº¥n luyá»‡n & hiá»‡u chá»‰nh), vá»›i phÃ¢n phá»‘i dá»± Ä‘oÃ¡n 
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
âˆˆ
Î”
ğ¶
âˆ’
1
p
(m)
(x)âˆˆÎ”
Câˆ’1
.

Há»c gating theo máº«u 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
âˆˆ
Î”
ğ‘€
âˆ’
1
w(x;Î¸)âˆˆÎ”
Mâˆ’1
 táº¡o ra

	
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
â€…â€Š
=
â€…â€Š
âˆ‘
ğ‘š
=
1
ğ‘€
ğ‘¤
ğ‘š
(
ğ‘¥
;
ğœƒ
)
â€‰
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
.
		
(2)
p
Q
Î¸
	â€‹

	â€‹

(x)=
m=1
âˆ‘
M
	â€‹

w
m
	â€‹

(x;Î¸)p
(m)
(x).
(2)

Suy luáº­n abstention theo cÃ¢n báº±ng nhÃ³m báº±ng quy táº¯c Bayes cá»§a bÃ i toÃ¡n tuyáº¿n tÃ­nh (bÃªn dÆ°á»›i) cáº¯m 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

 vÃ o (thay 
ğœ‚
Î·).

Huáº¥n luyá»‡n 
ğœƒ
Î¸ khÃ´ng tá»‘i Æ°u loss trá»±c tiáº¿p mÃ  tá»‘i Æ°u má»™t bound PAC-Bayes cho (1), nháº±m khá»‘ng cháº¿ overfit (Ä‘áº·c biá»‡t á»Ÿ tail).

2) Contributions (viáº¿t tháº³ng vÃ o abstract/introduction)

Bound PAC-Bayes cho balanced selective risk. ChÃºng tÃ´i phÃ¡t biá»ƒu vÃ  chá»©ng minh má»™t tá»•ng quÃ¡t hoÃ¡ PAC-Bayes cho rá»§i ro (1) thÃ´ng qua má»™t biá»ƒu thá»©c tuyáº¿n tÃ­nh tÆ°Æ¡ng Ä‘Æ°Æ¡ng cÃ³ rÃ ng buá»™c, cho phÃ©p Ã¡p dá»¥ng trá»±c tiáº¿p vÃ o mÃ´ hÃ¬nh cÃ³ reject vÃ  chia nhÃ³m, khÃ´ng cáº§n surrogate.

Gating-by-Bound. Äá» xuáº¥t thuáº­t toÃ¡n há»c gating 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) báº±ng cÃ¡ch tá»‘i thiá»ƒu hoÃ¡ cáº­n trÃªn PAC-Bayes cá»§a balanced selective risk; trong vÃ²ng trong dÃ¹ng oracle rule (Theorem 1 trong cÃ´ng trÃ¬nh gá»‘c) vá»›i 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

.

Group-aware prior/budget. Thiáº¿t káº¿ prior 
Î 
Î  (hoáº·c ngÃ¢n sÃ¡ch KL) phá»¥ thuá»™c nhÃ³m (head/tail), giÃºp Ä‘áº©y nÄƒng lá»±c ensemble vá» tail má»™t cÃ¡ch cÃ³ nguyÃªn lÃ½.

Báº£o Ä‘áº£m lÃ½ thuyáº¿t. (i) Plug-in optimality: vá»›i 
ğ›¼
,
ğœ‡
Î±,Î¼ cá»‘ Ä‘á»‹nh, quy táº¯c chá»n lá»›p/abstain tá»‘i Æ°u trÃªn lá»›p rule dá»±a trÃªn 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

 cÃ³ dáº¡ng Ä‘Ã³ng (cÃ¹ng cáº¥u trÃºc vá»›i Theorem 1); (ii) PAC-Bayes bound vá»›i loss bá»‹ cháº·n; (iii) má»Ÿ rá»™ng worst-group vá»›i no-regret EG (Äƒn khá»›p Theorem 3 â€œmixture of abstainersâ€).

3) Method
3.1. Tuyáº¿n tÃ­nh hoÃ¡ rá»§i ro cÃ¢n báº±ng (dÃ¹ng biáº¿n phá»¥)

Äáº·t

	
ğ›¼
ğ‘˜
â€…â€Š
â‰œ
â€…â€Š
ğ¾
â€‰
Pr
â¡
(
ğ‘Ÿ
(
ğ‘¥
)
=
0
,
â€…â€Š
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
,
ğ›¼
ğ‘˜
âˆˆ
(
0
,
1
]
.
		
(3)
Î±
k
	â€‹

â‰œKPr(r(x)=0,yâˆˆG
k
	â€‹

),Î±
k
	â€‹

âˆˆ(0,1].
(3)

Khi Ä‘Ã³ (chuáº©n hoÃ¡ nhÆ° trong cÃ´ng trÃ¬nh gá»‘c), rá»§i ro cÃ¢n báº±ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i má»¥c tiÃªu tuyáº¿n tÃ­nh cÃ³ rÃ ng buá»™c:

	
âˆ‘
ğ‘˜
=
1
ğ¾
1
ğ›¼
ğ‘˜
Pr
â¡
(
ğ‘¦
â‰ 
â„
(
ğ‘¥
)
,
â€‰
ğ‘Ÿ
(
ğ‘¥
)
=
0
,
â€‰
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
â€…â€Š
+
â€…â€Š
ğ‘
â€‰
Pr
â¡
(
ğ‘Ÿ
(
ğ‘¥
)
=
1
)
âŸ
ğ‘…
ğ›¼
lin
(
â„
,
ğ‘Ÿ
)
s.t.
ğ›¼
ğ‘˜
â€…â€Š
=
â€…â€Š
ğ¾
Pr
â¡
(
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
.
		
(4)
R
Î±
lin
	â€‹

(h,r)
k=1
âˆ‘
K
	â€‹

Î±
k
	â€‹

1
	â€‹

Pr(y
î€ 
=h(x),r(x)=0,yâˆˆG
k
	â€‹

)+cPr(r(x)=1)
	â€‹

	â€‹

s.t.Î±
k
	â€‹

=KPr(r=0,yâˆˆG
k
	â€‹

).
(4)

(4) lÃ  Ä‘Ãºng theo phÃ©p tÃ¡ch máº«u sá»‘/biáº¿n phá»¥ tiÃªu chuáº©n; nghiá»‡m thoáº£ KKT cho 
ğ›¼
ğ‘˜
Î±
k
	â€‹

 khÃ´i phá»¥c (1). Ta sáº½ dÃ¹ng (4) Ä‘á»ƒ vá»«a suy luáº­n Bayes, vá»«a â€œÄ‘Æ°a loss vá» dáº¡ng bá»‹ cháº·nâ€ cho PAC-Bayes.

3.2. Quy táº¯c Bayes plug-in trÃªn 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹


Vá»›i 
ğœ‚
(
â‹…
)
Î·(â‹…) chÆ°a biáº¿t, ta plug-in 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x) (2) vÃ o quy táº¯c Bayes tá»‘i Æ°u cá»§a (4). Cá»¥ thá»ƒ, Ä‘áº·t chá»‰ sá»‘ nhÃ³m 
[
ğ‘¦
]
[y] cho lá»›p 
ğ‘¦
y vÃ  cÃ¡c bá»™i sá»‘ Lagrange 
ğœ‡
ğ‘˜
Î¼
k
	â€‹

, ta cÃ³:

Äá»‹nh lÃ½ 1 (Plug-in optimality trÃªn 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

).
Cho 
ğ›¼
âˆˆ
(
0
,
1
]
ğ¾
Î±âˆˆ(0,1]
K
 vÃ  
ğœ‡
âˆˆ
ğ‘…
ğ¾
Î¼âˆˆR
K
 cá»‘ Ä‘á»‹nh. XÃ©t cáº·p rule 
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
(h
Î¸
	â€‹

,r
Î¸
	â€‹

) xÃ¡c Ä‘á»‹nh bá»Ÿi

	
â„
ğœƒ
(
ğ‘¥
)
â€…â€Š
=
â€…â€Š
arg
â¡
max
â¡
ğ‘¦
â€…â€Š
ğ‘
ğ‘„
ğœƒ
,
ğ‘¦
(
ğ‘¥
)
ğ›¼
[
ğ‘¦
]
,
		
(5a)
h
Î¸
	â€‹

(x)=arg
y
max
	â€‹

Î±
[y]
	â€‹

p
Q
Î¸
	â€‹

,y
	â€‹

(x)
	â€‹

,
(5a)
	
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
1
Â 
âŸº
Â 
max
â¡
ğ‘¦
ğ‘
ğ‘„
ğœƒ
,
ğ‘¦
(
ğ‘¥
)
ğ›¼
[
ğ‘¦
]
â€…â€Š
<
â€…â€Š
âˆ‘
ğ‘—
â€‰â£
(
1
ğ›¼
[
ğ‘—
]
âˆ’
ğœ‡
[
ğ‘—
]
)
ğ‘
ğ‘„
ğœƒ
,
ğ‘—
(
ğ‘¥
)
â€…â€Š
âˆ’
â€…â€Š
ğ‘
.
		
(5b)
r
Î¸
	â€‹

(x)=1Â âŸºÂ 
y
max
	â€‹

Î±
[y]
	â€‹

p
Q
Î¸
	â€‹

,y
	â€‹

(x)
	â€‹

<
j
âˆ‘
	â€‹

(
Î±
[j]
	â€‹

1
	â€‹

âˆ’Î¼
[j]
	â€‹

)p
Q
Î¸
	â€‹

,j
	â€‹

(x)âˆ’c.
(5b)

Khi Ä‘Ã³, trong lá»›p cÃ¡c rule chá»‰ phá»¥ thuá»™c vÃ o 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

, 
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
(h
Î¸
	â€‹

,r
Î¸
	â€‹

) tá»‘i Æ°u 
ğ‘…
ğ›¼
lin
R
Î±
lin
	â€‹

 á»Ÿ (4).
PhÃ¡c hoáº¡ chá»©ng minh. Láº­p Lagrangian cá»§a (4) theo cÃ¡ch chuáº©n; tá»‘i Æ°u cá»¥c bá»™ theo tá»«ng 
ğ‘¥
x dáº«n Ä‘áº¿n chá»n lá»›p báº±ng 
arg
â¡
max
â¡
ğ‘¦
ğ‘
ğ‘„
ğœƒ
,
ğ‘¦
/
ğ›¼
[
ğ‘¦
]
argmax
y
	â€‹

p
Q
Î¸
	â€‹

,y
	â€‹

/Î±
[y]
	â€‹

 vÃ  so sÃ¡nh cháº¥p nháº­n vs reject báº±ng ngÆ°á»¡ng tuyáº¿n tÃ­nh trÃªn toÃ n vector 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x) (nhÆ° (5b)). Äiá»ƒm máº¥u chá»‘t: dáº¡ng tuyáº¿n tÃ­nh theo 
ğ‘
p báº£o toÃ n dÆ°á»›i phÃ©p thay 
ğœ‚
â†¦
ğ‘
ğ‘„
ğœƒ
Î·â†¦p
Q
Î¸
	â€‹

	â€‹

.

Ghi chÃº. Khi 
ğ›¼
,
ğœ‡
Î±,Î¼ Ä‘Æ°á»£c thay bá»Ÿi nghiá»‡m cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm (Æ°á»›c lÆ°á»£ng tá»« táº­p hiá»‡u chá»‰nh), (5) chÃ­nh lÃ  quy táº¯c suy luáº­n cá»§a PB-GSE.

3.3. Huáº¥n luyá»‡n 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) báº±ng PAC-Bayes

Ta xem 
ğœƒ
Î¸ lÃ  tham sá»‘ cá»§a gating; xÃ¡c suáº¥t hiá»‡p biáº¿n cá»§a 
ğœƒ
Î¸ Ä‘Æ°á»£c mÃ´ hÃ¬nh hoÃ¡ bá»Ÿi posterior 
ğ‘„
Q (vÃ­ dá»¥ Gaussian) vÃ  prior 
Î 
Î  (Gaussian chuáº©n, hay prior nhÃ³m-nháº¡y). Vá»›i 
ğœƒ
âˆ¼
ğ‘„
Î¸âˆ¼Q, predictor lÃ  rule 
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
(h
Î¸
	â€‹

,r
Î¸
	â€‹

) theo (5). Äá»‹nh nghÄ©a loss bá»‹ cháº·n cho (4):

	
â„“
ğ›¼
(
ğœƒ
;
(
ğ‘¥
,
ğ‘¦
)
)
â€…â€Š
=
â€…â€Š
âˆ‘
ğ‘˜
=
1
ğ¾
1
ğ›¼
ğ‘˜
â€‰
1
[
ğ‘¦
âˆˆ
ğº
ğ‘˜
]
â€…â€Š
1
[
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
0
]
â€…â€Š
1
[
â„
ğœƒ
(
ğ‘¥
)
â‰ 
ğ‘¦
]
â€…â€Š
+
â€…â€Š
ğ‘
â€‰
1
[
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
1
]
.
		
(6)
â„“
Î±
	â€‹

(Î¸;(x,y))=
k=1
âˆ‘
K
	â€‹

Î±
k
	â€‹

1
	â€‹

1[yâˆˆG
k
	â€‹

]1[r
Î¸
	â€‹

(x)=0]1[h
Î¸
	â€‹

(x)
î€ 
=y]+c1[r
Î¸
	â€‹

(x)=1].
(6)

RÃµ rÃ ng 
0
â‰¤
â„“
ğ›¼
â‰¤
ğ¿
ğ›¼
:
=
max
â¡
{
ğ‘
,
â€‰
âˆ‘
ğ‘˜
1
ğ›¼
ğ‘˜
}
+
ğ‘
0â‰¤â„“
Î±
	â€‹

â‰¤L
Î±
	â€‹

:=max{c,âˆ‘
k
	â€‹

Î±
k
	â€‹

1
	â€‹

}+c. Ta chuáº©n hoÃ¡ 
â„“
~
ğ›¼
=
â„“
ğ›¼
/
ğ¿
ğ›¼
âˆˆ
[
0
,
1
]
â„“
~
Î±
	â€‹

=â„“
Î±
	â€‹

/L
Î±
	â€‹

âˆˆ[0,1].

4) Theory
4.1. PAC-Bayes bound cho balanced selective risk tuyáº¿n tÃ­nh

Äá»‹nh lÃ½ 2 (PAC-Bayes cho 
â„“
~
ğ›¼
â„“
~
Î±
	â€‹

).
Cho máº«u i.i.d. 
ğ‘†
=
{
(
ğ‘¥
ğ‘–
,
ğ‘¦
ğ‘–
)
}
ğ‘–
=
1
ğ‘›
âˆ¼
ğ·
ğ‘›
S={(x
i
	â€‹

,y
i
	â€‹

)}
i=1
n
	â€‹

âˆ¼D
n
. Vá»›i má»i prior 
Î 
Î  trÃªn 
ğœƒ
Î¸ (Ä‘á»™c láº­p 
ğ‘†
S), vá»›i má»i posterior 
ğ‘„
Q (cÃ³ thá»ƒ phá»¥ thuá»™c 
ğ‘†
S), vá»›i má»i 
ğ›¿
âˆˆ
(
0
,
1
)
Î´âˆˆ(0,1), vá»›i xÃ¡c suáº¥t 
â‰¥
1
âˆ’
ğ›¿
â‰¥1âˆ’Î´ trÃªn 
ğ‘†
S, ta cÃ³

	
ğ¸
ğœƒ
âˆ¼
ğ‘„
[
ğ‘…
ğ›¼
lin
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
]
â€…â€Š
â‰¤
â€…â€Š
ğ¸
ğœƒ
âˆ¼
ğ‘„
[
ğ‘…
^
ğ›¼
,
ğ‘†
lin
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
]
â€…â€Š
+
â€…â€Š
ğ¿
ğ›¼
K
L
(
ğ‘„
âˆ¥
Î 
)
+
ln
â¡
â€‰â£
2
ğ‘›
ğ›¿
2
(
ğ‘›
âˆ’
1
)
.
		
(7)
E
Î¸âˆ¼Q
	â€‹

[R
Î±
lin
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

)]â‰¤E
Î¸âˆ¼Q
	â€‹

[
R
Î±,S
lin
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

)]+L
Î±
	â€‹

2(nâˆ’1)
KL(Qâˆ¥Î )+ln
Î´
2
n
	â€‹

	â€‹

	â€‹

	â€‹

.
(7)

Trong Ä‘Ã³ 
ğ‘…
^
ğ›¼
,
ğ‘†
lin
R
Î±,S
lin
	â€‹

 lÃ  trung bÃ¬nh máº«u cá»§a 
â„“
ğ›¼
â„“
Î±
	â€‹

 (khÃ´ng chuáº©n hoÃ¡).
PhÃ¡c hoáº¡ chá»©ng minh. Ãp dá»¥ng PAC-Bayes cá»• Ä‘iá»ƒn (Seeger/Catoni) cho loss bá»‹ cháº·n 
â„“
~
ğ›¼
âˆˆ
[
0
,
1
]
â„“
~
Î±
	â€‹

âˆˆ[0,1], rá»“i nhÃ¢n láº¡i bá»Ÿi 
ğ¿
ğ›¼
L
Î±
	â€‹

. Quy táº¯c 
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
(h
Î¸
	â€‹

,r
Î¸
	â€‹

) lÃ  xÃ¡c Ä‘á»‹nh vá»›i 
ğœƒ
Î¸, nÃªn toÃ n bá»™ ngáº«u nhiÃªn náº±m á»Ÿ 
ğ‘„
Q.

BÃ¬nh luáº­n. (7) lÃ  cáº­n trÃªn rá»§i ro tuyáº¿n tÃ­nh 
ğ‘…
ğ›¼
lin
R
Î±
lin
	â€‹

. Khi 
ğ›¼
Î± chá»n Ä‘Ãºng nghiá»‡m KKT (hoáº·c Ä‘Æ°á»£c Æ°á»›c lÆ°á»£ng nháº¥t quÃ¡n), chÃªnh lá»‡ch giá»¯a 
ğ‘…
ğ›¼
lin
R
Î±
lin
	â€‹

 vÃ  balanced risk (1) lÃ  bÃ¹ trá»« nhá» (má»¥c 4.3). VÃ¬ váº­y tá»‘i thiá»ƒu hoÃ¡ váº¿ pháº£i (7) lÃ  há»£p lÃ½ cho balanced selective risk.

4.2. Group-aware prior/budget

Äá»ƒ Æ°u tiÃªn tail, cÃ³ hai Ä‘Æ°á»ng Ä‘Ãºng nguyÃªn lÃ½:

Prior nhÃ³m-nháº¡y. Äáº·t 
Î 
=
ğ‘
(
0
,
ğœ
2
ğ¼
)
Î =N(0,Ïƒ
2
I) nhÆ°ng chuáº©n hoÃ¡ target cá»§a gating sao cho khi 
ğ‘¥
x thuá»™c vÃ¹ng tail (phÃ¡t hiá»‡n báº±ng meta-Ä‘áº·c trÆ°ng hoáº·c nhÃ£n nhÃ³m), máº¡ng gating cÃ³ Ä‘á»™ tá»± do lá»›n hÆ¡n (vÃ­ dá»¥ thÃªm feat vÃ o 
ğ‘¤
w), Ä‘á»“ng thá»i giá»¯ 
ğœ
Ïƒ lá»›n hÆ¡n â€” lÃ m KL(Q|\Pi) nhá» hÆ¡n khi 
ğ‘„
Q dá»‹ch chuyá»ƒn Ä‘á»ƒ phá»¥c vá»¥ tail.

KL-budget theo nhÃ³m. Tá»‘i Æ°u (7) vá»›i rÃ ng buá»™c 
ğ¸
(
ğ‘¥
,
ğ‘¦
)
âˆˆ
ğ‘†
âˆ©
ğº
ğ‘˜
K
L
(
ğ‘„
âˆ¥
Î 
)
â‰¤
ğµ
ğ‘˜
E
(x,y)âˆˆSâˆ©G
k
	â€‹

	â€‹

KL(Qâˆ¥Î )â‰¤B
k
	â€‹

, vá»›i 
ğµ
tail
>
ğµ
head
B
tail
	â€‹

>B
head
	â€‹

. Äiá»u nÃ y cho nÄƒng lá»±c biá»ƒu diá»…n gating lá»›n hÆ¡n á»Ÿ tail nhÆ°ng váº«n trong biÃªn PAC-Bayes.

4.3. Tá»« rá»§i ro tuyáº¿n tÃ­nh vá» balanced risk

Khi 
ğ›¼
Î± lÃ  nghiá»‡m KKT 
ğ›¼
â‹†
=
ğ¾
Pr
â¡
(
ğ‘Ÿ
ğœƒ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
Î±
â‹†
=KPr(r
Î¸
	â€‹

=0,yâˆˆG
k
	â€‹

), ta cÃ³ 
ğ‘…
ğ›¼
â‹†
lin
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
=
ğ‘…
bal
rej
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
R
Î±
â‹†
lin
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

)=R
bal
rej
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

). Trong thá»±c hÃ nh, 
ğ›¼
Î± Æ°á»›c lÆ°á»£ng tá»« má»™t táº­p hiá»‡u chá»‰nh Ä‘á»™c láº­p 
ğ‘†
cal
S
cal
	â€‹

 báº±ng cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm:

	
ğ›¼
^
ğ‘˜
â€…â€Š
=
â€…â€Š
ğ¾
â€‰
Pr
â¡
ğ‘†
cal
â€‰â£
(
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
0
,
â€…â€Š
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
.
		
(8)
Î±
^
k
	â€‹

=K
S
cal
	â€‹

Pr
	â€‹

(r
Î¸
	â€‹

(x)=0,yâˆˆG
k
	â€‹

).
(8)

Giáº£ sá»­ 
ğ›¼
^
â†’
ğ›¼
â‹†
Î±
^
â†’Î±
â‹†
 theo xÃ¡c suáº¥t khi 
âˆ£
ğ‘†
cal
âˆ£
â†’
âˆ
âˆ£S
cal
	â€‹

âˆ£â†’âˆ (Ä‘iá»u kiá»‡n Ä‘á»u hoÃ¡ nháº¹). Khi Ä‘Ã³

	
âˆ£
ğ‘…
bal
rej
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
âˆ’
ğ‘…
ğ›¼
^
lin
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
âˆ£
â€…â€Š
â‰¤
â€…â€Š
âˆ‘
ğ‘˜
=
1
ğ¾
âˆ£
1
ğ›¼
ğ‘˜
â‹†
âˆ’
1
ğ›¼
^
ğ‘˜
âˆ£
â€‰
Pr
â¡
(
ğ‘¦
â‰ 
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
â€…â€Š
â†’
ğ‘
â€…â€Š
0.
		
(9)
	â€‹

R
bal
rej
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

)âˆ’R
Î±
^
lin
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

)
	â€‹

â‰¤
k=1
âˆ‘
K
	â€‹

	â€‹

Î±
k
â‹†
	â€‹

1
	â€‹

âˆ’
Î±
^
k
	â€‹

1
	â€‹

	â€‹

Pr(y
î€ 
=h
Î¸
	â€‹

,r
Î¸
	â€‹

=0,yâˆˆG
k
	â€‹

)
p
	â€‹

0.
(9)

VÃ¬ váº­y, tá»‘i Æ°u bound (7) vá»›i 
ğ›¼
=
ğ›¼
^
Î±=
Î±
^
 lÃ  nháº¥t quÃ¡n cho balanced risk.

4.4. Worst-group extension (mixture of abstainers)

Cho 
ğœ“
(
ğ‘’
1
,
â€¦
,
ğ‘’
ğ¾
)
=
max
â¡
ğ‘˜
ğ‘’
ğ‘˜
Ïˆ(e
1
	â€‹

,â€¦,e
K
	â€‹

)=max
k
	â€‹

e
k
	â€‹

. Äáº·t trá»ng sá»‘ nhÃ³m 
ğ›½
âˆˆ
Î”
ğ¾
âˆ’
1
Î²âˆˆÎ”
Kâˆ’1
. Vá»›i má»—i 
ğ›½
Î², ta tá»‘i Æ°u rá»§i ro 
âˆ‘
ğ‘˜
ğ›½
ğ‘˜
ğ‘’
ğ‘˜
âˆ‘
k
	â€‹

Î²
k
	â€‹

e
k
	â€‹

 báº±ng (5)â€“(7) (thay loss theo 
ğ›½
Î²). VÃ²ng ngoÃ i cáº­p nháº­t 
ğ›½
Î² báº±ng Exponentiated-Gradient (EG) (no-regret). Káº¿t quáº£ chuáº©n: sau 
ğ‘‡
T vÃ²ng, nghiá»‡m ngáº«u nhiÃªn (hoáº·c trung bÃ¬nh) Ä‘áº¡t sai sá»‘ tá»‘i Æ°u 
ğ‘‚
(
log
â¡
ğ¾
/
ğ‘‡
)
O(
logK/T
	â€‹

) cá»™ng cÃ¡c sai sá»‘ Æ°á»›c lÆ°á»£ng trong vÃ²ng trong (PAC-Bayes).

5) Thuáº­t toÃ¡n (tá»‘i thiá»ƒu bound)

Input. Base posteriors 
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
(m)
(x), táº­p train 
ğ‘†
tr
S
tr
	â€‹

, táº­p hiá»‡u chá»‰nh 
ğ‘†
cal
S
cal
	â€‹

, táº­p Ä‘Ã¡nh giÃ¡ 
ğ‘†
S (Ä‘á»™c láº­p cho bound), chi phÃ­ reject 
ğ‘
c.

BÆ°á»›c 0. Huáº¥n luyá»‡n/cÃ¢n báº±ng base models; calibrate (temperature / vector scaling) Ä‘á»ƒ 
ğ‘
(
ğ‘š
)
p
(m)
 tin cáº­y theo nhÃ³m.

BÆ°á»›c 1 (Khá»Ÿi táº¡o). Chá»n prior 
Î 
Î  (Gaussian 
ğ‘
(
0
,
ğœ
2
ğ¼
)
N(0,Ïƒ
2
I) hoáº·c prior nhÃ³m-nháº¡y). Khá»Ÿi táº¡o posterior 
ğ‘„
=
ğ‘
(
ğœƒ
0
,
ğ‘ 
0
2
ğ¼
)
Q=N(Î¸
0
	â€‹

,s
0
2
	â€‹

I).

BÆ°á»›c 2 (Cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
Î±). Vá»›i 
ğœƒ
âˆ¼
ğ‘„
Î¸âˆ¼Q (láº¥y máº«u hoáº·c dÃ¹ng trung bÃ¬nh), tÃ­nh 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

 (2); suy ra 
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
(h
Î¸
	â€‹

,r
Î¸
	â€‹

) theo (5); cáº­p nháº­t 
ğ›¼
^
Î±
^
 theo (8) trÃªn 
ğ‘†
cal
S
cal
	â€‹

. Láº·p Ä‘áº¿n há»™i tá»¥ (10â€“20 vÃ²ng; nhanh).

BÆ°á»›c 3 (Minimise PAC-Bayes upper-bound). TrÃªn táº­p 
ğ‘†
S (Ä‘á»™c láº­p 
ğ‘†
cal
S
cal
	â€‹

), tá»‘i thiá»ƒu hoÃ¡

	
ğ‘ˆ
(
ğ‘„
)
â€…â€Š
=
â€…â€Š
ğ¸
ğœƒ
âˆ¼
ğ‘„
[
ğ‘…
^
ğ›¼
^
,
ğ‘†
lin
(
â„
ğœƒ
,
ğ‘Ÿ
ğœƒ
)
]
â€…â€Š
+
â€…â€Š
ğ¿
ğ›¼
^
K
L
(
ğ‘„
âˆ¥
Î 
)
+
ln
â¡
â€‰â£
2
ğ‘›
ğ›¿
2
(
ğ‘›
âˆ’
1
)
â€…â€Š
+
â€…â€Š
ğœ†
â€‰
Î©
fair
(
ğ‘„
)
,
		
(10)
U(Q)=E
Î¸âˆ¼Q
	â€‹

[
R
Î±
^
,S
lin
	â€‹

(h
Î¸
	â€‹

,r
Î¸
	â€‹

)]+L
Î±
^
	â€‹

2(nâˆ’1)
KL(Qâˆ¥Î )+ln
Î´
2
n
	â€‹

	â€‹

	â€‹

	â€‹

+Î»Î©
fair
	â€‹

(Q),
(10)

vá»›i 
Î©
fair
Î©
fair
	â€‹

 (tuá»³ chá»n) pháº¡t lá»‡ch accept-rate giá»¯a nhÃ³m. Thá»±c táº¿, ta dÃ¹ng reparameterization (Gaussian 
ğ‘„
Q) Ä‘á»ƒ Æ°á»›c lÆ°á»£ng gradient Monte-Carlo cá»§a váº¿ trÃ¡i (10); 
K
L
KL cÃ³ dáº¡ng Ä‘Ã³ng.

BÆ°á»›c 4 (Inference). DÃ¹ng 
ğ‘„
Q cuá»‘i (hoáº·c trung bÃ¬nh) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸), tÃ­nh 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x), Ã¡p dá»¥ng (5) vá»›i 
ğ›¼
^
,
ğœ‡
^
Î±
^
,
Î¼
^
	â€‹

 (tá»« vÃ²ng cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm) â†’ dá»± Ä‘oÃ¡n/abstain.

Worst-group (tuá»³ chá»n). VÃ²ng ngoÃ i cáº­p nháº­t 
ğ›½
Î² (EG), má»—i láº§n gá»i BÆ°á»›c 2â€“3 vá»›i loss theo 
ğ›½
Î².

6) Báº£o Ä‘áº£m Ä‘Ãºng Ä‘áº¯n & cÃ¡c lÆ°u Ã½ thá»±c hÃ nh

Loss bá»‹ cháº·n. (6) bá»‹ cháº·n há»¯u háº¡n 
â‡’
â‡’ Ã¡p dá»¥ng PAC-Bayes chuáº©n.

Äá»™c láº­p máº«u cho bound. Äá»ƒ (7) â€œsáº¡châ€: dÃ¹ng data-splitâ€”
ğ›¼
^
Î±
^
 há»c trÃªn 
ğ‘†
cal
S
cal
	â€‹

; bound (7) Ä‘Ã¡nh giÃ¡ trÃªn 
ğ‘†
S Ä‘á»™c láº­p. Khi cáº§n one-split, dÃ¹ng ká»¹ thuáº­t ghost sample hoáº·c cross-fitting (trÃ¬nh bÃ y trong phá»¥ lá»¥c).

Gating lÃ  xÃ¡c Ä‘á»‹nh theo 
ğœƒ
Î¸. PAC-Bayes xem ngáº«u nhiÃªn á»Ÿ 
ğ‘„
Q (khÃ´ng á»Ÿ 
ğ‘¥
x) â†’ hoÃ n toÃ n phÃ¹ há»£p: má»i phá»¥ thuá»™c 
ğ‘¥
x náº±m trong hÃ m 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) xÃ¡c Ä‘á»‹nh.

Hiá»‡u chá»‰nh xÃ¡c suáº¥t. VÃ¬ (5) lÃ  tuyáº¿n tÃ­nh theo 
ğ‘
p, calibration theo nhÃ³m (temperature/vector scaling) trÆ°á»›c khi há»c gating giÃºp giáº£m nhiá»…u ngÆ°á»¡ng.

Tá»‘i Æ°u hoÃ¡. Trong thá»±c nghiá»‡m, cÃ³ thá»ƒ dÃ¹ng Dirac-posterior (deterministic 
ğœƒ
Î¸) + regularizer KL surrogate (weight decay nhÃ³m-nháº¡y) Ä‘á»ƒ gáº§n vá»›i (10) khi compute háº¡n cháº¿; bound chÃ­nh thá»©c yÃªu cáº§u 
ğ‘„
Q thá»±c.

7) PhÃ¡c tháº£o chá»©ng minh (táº­p trung cÃ¡c Ä‘iá»ƒm dá»… sai)

(i) TÆ°Æ¡ng Ä‘Æ°Æ¡ng (1)â†”(4). Viáº¿t 
Pr
â¡
(
ğ‘¦
â‰ 
â„
âˆ£
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
=
Pr
â¡
(
ğ‘¦
â‰ 
â„
,
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
/
Pr
â¡
(
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
Pr(y
î€ 
=hâˆ£r=0,yâˆˆG
k
	â€‹

)=Pr(y
î€ 
=h,r=0,yâˆˆG
k
	â€‹

)/Pr(r=0,yâˆˆG
k
	â€‹

). Äáº·t 
ğ›¼
ğ‘˜
=
ğ¾
Pr
â¡
(
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
Î±
k
	â€‹

=KPr(r=0,yâˆˆG
k
	â€‹

) 
â‡’
â‡’ (4). Nghiá»‡m KKT khÃ´i phá»¥c (1) táº¡i 
ğ›¼
â‹†
Î±
â‹†
.

(ii) Plug-in optimality (Äá»‹nh lÃ½ 1). Lagrangian cá»§a (4):

ğ¿
=
âˆ‘
ğ‘˜
1
ğ›¼
ğ‘˜
Pr
â¡
(
ğ‘¦
â‰ 
â„
,
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
+
ğ‘
Pr
â¡
(
ğ‘Ÿ
=
1
)
+
âˆ‘
ğ‘˜
ğœ‡
ğ‘˜
(
ğ¾
Pr
â¡
(
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
âˆ’
ğ›¼
ğ‘˜
)
.
L=
k
âˆ‘
	â€‹

Î±
k
	â€‹

1
	â€‹

Pr(y
î€ 
=h,r=0,yâˆˆG
k
	â€‹

)+cPr(r=1)+
k
âˆ‘
	â€‹

Î¼
k
	â€‹

(KPr(r=0,yâˆˆG
k
	â€‹

)âˆ’Î±
k
	â€‹

).

PhÃ¢n rÃ£ theo 
ğ‘¥
x vÃ  thay 
ğœ‚
Î· bá»Ÿi 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

 cho ta hÃ m má»¥c tiÃªu tuyáº¿n tÃ­nh theo 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x); tá»‘i Æ°u cá»¥c bá»™ sinh (5a)(5b).

(iii) PAC-Bayes (Äá»‹nh lÃ½ 2). Ãp dá»¥ng báº¥t Ä‘áº³ng thá»©c PAC-Bayes cho loss bá»‹ cháº·n 
â„“
~
ğ›¼
âˆˆ
[
0
,
1
]
â„“
~
Î±
	â€‹

âˆˆ[0,1]:

ğ¸
ğ‘„
ğ‘…
â‰¤
ğ¸
ğ‘„
ğ‘…
^
+
(
K
L
(
ğ‘„
âˆ¥
Î 
)
+
ln
â¡
(
2
ğ‘›
/
ğ›¿
)
)
/
2
(
ğ‘›
âˆ’
1
)
E
Q
	â€‹

Râ‰¤E
Q
	â€‹

R
^
+
(KL(Qâˆ¥Î )+ln(2
n
	â€‹

/Î´))/2(nâˆ’1)
	â€‹

, sau Ä‘Ã³ nhÃ¢n ngÆ°á»£c 
ğ¿
ğ›¼
L
Î±
	â€‹

.

(iv) Nháº¥t quÃ¡n (9). DÆ°á»›i Ä‘iá»u kiá»‡n Ä‘á»u hoÃ¡ nháº¹ (vÃ­ dá»¥ 
inf
â¡
ğ‘˜
ğ›¼
ğ‘˜
â‹†
>
ğœ–
>
0
inf
k
	â€‹

Î±
k
â‹†
	â€‹

>Ïµ>0), luáº­t sá»‘ lá»›n cho 
ğ›¼
^
Î±
^
 â‡’ 
ğ›¼
^
â†’
ğ›¼
â‹†
Î±
^
â†’Î±
â‹†
, vÃ  vÃ¬ 
ğ‘¥
â†¦
1
/
ğ›¼
xâ†¦1/Î± liÃªn tá»¥c trÃªn 
[
ğœ–
,
1
]
[Ïµ,1], chÃªnh lá»‡ch co láº¡i vá» 0.

(v) Worst-group. EG lÃ  no-regret; khi vÃ²ng trong tráº£ vá» lá»i giáº£i xáº¥p xá»‰ tá»‘i Æ°u theo 
ğ›½
Î², nghiá»‡m há»—n há»£p (trung bÃ¬nh theo vÃ²ng láº·p) xáº¥p xá»‰ chuáº©n Bayes há»—n há»£p (Theorem 3 kiá»ƒu mixture-of-abstainers).

8) KhuÃ´n thá»±c nghiá»‡m (tá»‘i thiá»ƒu Ä‘á»ƒ publish)

Datasets: CIFAR-LT (imbalance 100/200), ImageNet-LT, iNaturalist-LT.

Baselines: CE+Chow; Balanced-CE+Chow; Bayes-plug-in (paper gá»‘c); Deep Ensemble + Chow; Conformal selective (khÃ´ng LT).

PB-GSE configs:

Base: cRT, LDAM-DRW, CB-Focal (Ä‘a dáº¡ng kiáº¿n trÃºc/sampling).

Calibration per-group.

Prior 
Î 
Î : Gaussian; phiÃªn báº£n group-aware (Ïƒ lá»›n hÆ¡n cho tail).

Posterior 
ğ‘„
Q: Gaussian (diagonal) trÃªn 
ğœƒ
Î¸ cá»§a gating (MLP nhá»).

Cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
^
Î±
^
 trÃªn 
ğ‘†
cal
S
cal
	â€‹

.

Tá»‘i thiá»ƒu (10) trÃªn 
ğ‘†
S (Ä‘á»™c láº­p), chá»n 
ğ›¿
=
0.05
Î´=0.05.

Metrics: balanced selective error @coverage, worst-group selective error, AURC, accept-rate@group, ECE@group.

Ablations: vai trÃ² prior nhÃ³m-nháº¡y, cá»¡ 
ğ‘„
Q (variance), áº£nh hÆ°á»Ÿng calibration, cÃ³/khÃ´ng fair regularizer, outer EG.

TL;DR cÃ¡ch triá»ƒn khai an toÃ n

Huáº¥n luyá»‡n M base models chÃº trá»ng tail; calibrate theo nhÃ³m.

Há»c gating 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) báº±ng cÃ¡ch tá»‘i thiá»ƒu hoÃ¡ PAC-Bayes upper-bound (7)/(10) cho rá»§i ro tuyáº¿n tÃ­nh (4); 
ğ›¼
Î± Æ°á»›c lÆ°á»£ng báº±ng cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm trÃªn táº­p hiá»‡u chá»‰nh Ä‘á»™c láº­p.

Suy luáº­n báº±ng quy táº¯c (5) (cÃ¹ng cáº¥u trÃºc Bayes-optimal cá»§a cÃ´ng trÃ¬nh gá»‘c), cÃ³ thá»ƒ má»Ÿ rá»™ng worst-group báº±ng EG.

náº¿u báº¡n muá»‘n, mÃ¬nh sáº½ chuyá»ƒn toÃ n bá»™ pháº§n â€œTheoryâ€ thÃ nh LaTeX (Ä‘á»‹nh lÃ½, chá»©ng minh, bá»• Ä‘á» & giáº£ thiáº¿t Ä‘áº§y Ä‘á»§) + pseudo-code cho vÃ²ng tá»‘i Æ°u (10) kÃ¨m cáº¥u hÃ¬nh 
ğ‘„
,
Î 
Q,Î  (Gaussian) Ä‘á»ƒ báº¡n tháº£ tháº³ng vÃ o báº£n tháº£o.

You said:
TÃ´i muá»‘n hÆ°á»›ng dáº«n triá»ƒn khai implement thá»±c nghiá»‡m chi tiáº¿t cá»¥ thá»ƒ, Ä‘á»ƒ cÃ³ thá»ƒ viáº¿t thÃ nh paper. Cá»¥ thá»ƒ tá»« dá»¯ liá»‡u, xá»­ lÃ½ dá»¯ liá»‡u, Ä‘Ã¡nh giÃ¡ metrics, .. 
ChatGPT said:
Thought for 1m 44s

Tuyá»‡tâ€”dÆ°á»›i Ä‘Ã¢y lÃ  â€œplaybook triá»ƒn khaiâ€ cho PB-GSE: PAC-Bayes Group-Selective Ensemble Ä‘á»§ chi tiáº¿t Ä‘á»ƒ báº¡n code, cháº¡y thÃ­ nghiá»‡m vÃ  viáº¿t paper. MÃ¬nh chia theo: dá»¯ liá»‡u â†’ tiá»n xá»­ lÃ½ â†’ huáº¥n luyá»‡n base models (long-tail aware) â†’ hiá»‡u chá»‰nh (calibration) â†’ huáº¥n luyá»‡n gating theo bound PAC-Bayes â†’ suy luáº­n theo quy táº¯c Theorem-1 plug-in vá»›i cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
,
ğœ‡
Î±,Î¼ â†’ má»Ÿ rá»™ng worst-group â†’ Ä‘Ã¡nh giÃ¡ & bÃ¡o cÃ¡o. ToÃ n bá»™ Ä‘á»u Ä‘á»™c láº­p cÃ´ng cá»¥ (PyTorch/Lightning/Hydra Ä‘á»u Ã¡p Ä‘Æ°á»£c).

0) Tá»•ng quan pipeline

Chuáº©n bá»‹ dataset LT + chia nhÃ³m (head/tail/â€¦); tÃ¡ch train/val/cal/test.

Huáº¥n luyá»‡n M base models (Ä‘a dáº¡ng loss/sampling/augment) + calibration theo nhÃ³m â†’ lÆ°u xÃ¡c suáº¥t 
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
(m)
(x) cho {cal, val, test}.

Huáº¥n luyá»‡n gating 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) Ä‘á»ƒ táº¡o 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
=
âˆ‘
ğ‘š
ğ‘¤
ğ‘š
(
ğ‘¥
;
ğœƒ
)
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x)=âˆ‘
m
	â€‹

w
m
	â€‹

(x;Î¸)p
(m)
(x) báº±ng cÃ¡ch tá»‘i thiá»ƒu hoÃ¡ cáº­n PAC-Bayes cho rá»§i ro tuyáº¿n tÃ­nh cÃ¢n báº±ng (khÃ´ng tá»‘i Æ°u loss trá»±c tiáº¿p).

TÃ¬m 
ğ›¼
^
,
ğœ‡
^
Î±
^
,
Î¼
^
	â€‹

 báº±ng cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm trÃªn táº­p cal (Ä‘á»™c láº­p) theo quy táº¯c plug-in (Theorem-1).

Suy luáº­n trÃªn val/test: Ã¡p quy táº¯c plug-in (Theorem-1) vá»›i 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

, 
ğ›¼
^
,
ğœ‡
^
Î±
^
,
Î¼
^
	â€‹

; náº¿u lÃ m worst-group thÃ¬ thÃªm vÃ²ng EG bÃªn ngoÃ i.

ÄÃ¡nh giÃ¡: balanced selective error / worst-group selective error, AURC, coverage & accept-rate@group, ECE@group; ablation & phÃ¢n tÃ­ch.

1) Dá»¯ liá»‡u & chia nhÃ³m
1.1 Datasets khuyáº¿n nghá»‹

CIFAR-10-LT, CIFAR-100-LT: táº¡o LT báº±ng quy luáº­t 
ğ‘›
ğ‘¦
=
ğ‘›
max
â¡
â‹…
ğœŒ
ğ‘Ÿ
ğ‘¦
ğ¶
âˆ’
1
n
y
	â€‹

=n
max
	â€‹

â‹…Ï
Câˆ’1
r
y
	â€‹

	â€‹

 (imbalance factor 
ğœŒ
âˆˆ
{
0.01
,
0.005
}
Ïâˆˆ{0.01,0.005} tÆ°Æ¡ng á»©ng IF=100/200).

ImageNet-LT, iNaturalist-LT: dÃ¹ng split chÃ­nh thá»©c (khÃ´ng cáº§n sinh LT).

1.2 Chia táº­p & â€œgroupingâ€

Split: train / cal / val / test (vÃ­ dá»¥ 80/10/5/5 cho CIFAR-LT; vá»›i ImageNet-LT dÃ¹ng val chÃ­nh thá»©c vÃ  tÃ¡ch cal tá»« train).

Groups:

2-group (head/tail): tail = lá»›p cÃ³ 
ğ‘›
ğ‘¦
â‰¤
ğœ
n
y
	â€‹

â‰¤Ï„ (CIFAR-LT thÆ°á»ng 
ğœ
âˆˆ
[
20
,
50
]
Ï„âˆˆ[20,50]).

3-group (head/medium/tail): phÃ¢n vá»‹ 33%/66% theo 
ğ‘›
ğ‘¦
n
y
	â€‹

.

LÆ°u Ã¡nh xáº¡ lá»›p â†’ nhÃ³m: 
[
ğ‘¦
]
âˆˆ
{
1
,
â€¦
,
ğ¾
}
[y]âˆˆ{1,â€¦,K}.

2) Tiá»n xá»­ lÃ½ & sampling

Augment train: RandAugment (N=2, M=9â€“12), MixUp=0.2â€“0.4, CutMix=0.2â€“0.4, RandomResizedCrop, ColorJitter nháº¹; test/val dÃ¹ng CenterCrop/Resize.

Class-Aware Sampling (CAS): chá»n lá»›p uniform rá»“i láº¥y máº«u trong lá»›p; hoáº·c square-root sampling 
âˆ
ğ‘›
ğ‘¦
âˆ
n
y
	â€‹

	â€‹

 náº¿u tail quÃ¡ nhá».

Balanced batch: má»—i batch â‰¥ 50% áº£nh tá»« non-head (medium+tail) Ä‘á»ƒ giáº£m â€œÄ‘Ã¨â€ head.

3) Huáº¥n luyá»‡n base models (Ä‘a dáº¡ng hoÃ¡)

Cháº¡y song song/tuáº§n tá»± 3â€“6 mÃ´ hÃ¬nh:

Model #1 (cRT + Balanced Softmax)
Stage-1: CE instance-balanced 200â€“300 epochs; Stage-2: freeze backbone, retrain linear head vá»›i Balanced Softmax hoáº·c Logit-Adjust (+
log
â¡
ğœ‹
ğ‘¦
logÏ€
y
	â€‹

).

Model #2 (LDAM-DRW)
Margin 
ğ‘š
ğ‘¦
=
ğ¶
/
ğ‘›
ğ‘¦
1
/
4
m
y
	â€‹

=C/n
y
1/4
	â€‹

; Deferred Re-Weighting ná»­a sau training.

Model #3 (CB-Focal)
Class-Balanced weight 
ğ‘¤
ğ‘¦
=
1
âˆ’
ğ›½
1
âˆ’
ğ›½
ğ‘›
ğ‘¦
w
y
	â€‹

=
1âˆ’Î²
n
y
	â€‹

1âˆ’Î²
	â€‹

 (
ğ›½
Î²=0.999), Focal 
ğ›¾
=
1
âˆ¼
2
Î³=1âˆ¼2.

(Tuá»³ chá»n) Kiáº¿n trÃºc khÃ¡c (ConvNeXt/ViT), MixUp/CutMix/Res khÃ¡c biá»‡t, seeds khÃ¡c Ä‘á»ƒ tÄƒng diversity.

Hyper gá»£i Ã½ (CIFAR-LT):
SGD-momentum 0.9, wd=5e-4, cosine LR 0.1â†’0, bs=256, epochs=200; EMA on. Vá»›i ImageNet-LT: bs=1024, epochs 180â€“300; RandAugment máº¡nh hÆ¡n; AMP fp16.

Káº¿t xuáº¥t: LÆ°u logits & probs 
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
(m)
(x) cho {cal, val, test} (Ä‘á»‹nh dáº¡ng npz/zarr), kÃ¨m id áº£nh, nhÃ£n, group.

4) Calibration theo nhÃ³m

TrÆ°á»›c khi ensemble/gating, calibrate tá»«ng model per-group (head vs tail):

Temperature scaling (per-group): tÃ¬m 
ğ‘‡
ğ‘”
>
0
T
g
	â€‹

>0 tá»‘i thiá»ƒu NLL trÃªn cal-group 
ğ‘”
g.

(Tuá»³ chá»n) Vector scaling cho cá»¥m lá»›p hay nháº§m.

Ãp 
ğ‘‡
ğ‘”
T
g
	â€‹

 khi suy ra 
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
(m)
(x) trÃªn má»i táº­p.

5) Gating 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) & Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o

Äáº§u vÃ o gating (trÃªn cal/val): vá»›i má»—i áº£nh 
ğ‘¥
x,

concat 
[
ğ‘
(
1
)
(
ğ‘¥
)
,
â€¦
,
ğ‘
(
ğ‘€
)
(
ğ‘¥
)
]
[p
(1)
(x),â€¦,p
(M)
(x)] (MC dim 
ğ‘€
â€‰â£
Ã—
â€‰â£
ğ¶
MÃ—C),

thÃªm meta-features: entropy tá»«ng model, max-prob tá»«ng model, variance/logit-disagreement, one-hot group 
[
ğ‘”
(
ğ‘¥
)
]
[g(x)] hoáº·c extra bit â€œis-tailâ€.

Máº¡ng gating: MLP 1â€“2 táº§ng (hidden 64â€“128), Softmax cuá»‘i Ä‘á»ƒ ra 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
âˆˆ
Î”
ğ‘€
âˆ’
1
w(x;Î¸)âˆˆÎ”
Mâˆ’1
.
Ensemble posterior: 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
=
âˆ‘
ğ‘š
ğ‘¤
ğ‘š
(
ğ‘¥
;
ğœƒ
)
â€‰
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x)=âˆ‘
m
	â€‹

w
m
	â€‹

(x;Î¸)p
(m)
(x).

6) Quy táº¯c plug-in (Theorem-1) & cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
,
ğœ‡
Î±,Î¼

Cho chi phÃ­ reject 
ğ‘
c. Vá»›i 
ğ‘
ğ‘„
ğœƒ
p
Q
Î¸
	â€‹

	â€‹

, Ä‘á»‹nh nghÄ©a:

Classifier: 
â„
ğœƒ
(
ğ‘¥
)
=
arg
â¡
max
â¡
ğ‘¦
ğ‘
ğ‘„
ğœƒ
,
ğ‘¦
(
ğ‘¥
)
ğ›¼
[
ğ‘¦
]
h
Î¸
	â€‹

(x)=argmax
y
	â€‹

Î±
[y]
	â€‹

p
Q
Î¸
	â€‹

,y
	â€‹

(x)
	â€‹

.

Rejector: reject náº¿u

max
â¡
ğ‘¦
ğ‘
ğ‘„
ğœƒ
,
ğ‘¦
(
ğ‘¥
)
ğ›¼
[
ğ‘¦
]
â€…â€Š
<
â€…â€Š
âˆ‘
ğ‘—
(
1
ğ›¼
[
ğ‘—
]
âˆ’
ğœ‡
[
ğ‘—
]
)
â€‰
ğ‘
ğ‘„
ğœƒ
,
ğ‘—
(
ğ‘¥
)
â€…â€Š
âˆ’
â€…â€Š
ğ‘
.
y
max
	â€‹

Î±
[y]
	â€‹

p
Q
Î¸
	â€‹

,y
	â€‹

(x)
	â€‹

<
j
âˆ‘
	â€‹

(
Î±
[j]
	â€‹

1
	â€‹

âˆ’Î¼
[j]
	â€‹

)p
Q
Î¸
	â€‹

,j
	â€‹

(x)âˆ’c.

Cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
Î± (trÃªn cal):

Khá»Ÿi táº¡o 
ğ›¼
ğ‘˜
(
0
)
=
1
Î±
k
(0)
	â€‹

=1 (hoáº·c theo tá»‰ lá»‡ nhÃ³m).

Cho trÆ°á»›c 
ğœ‡
Î¼ (xem dÆ°á»›i), láº·p 
ğ‘¡
=
0
â†’
ğ‘‡
t=0â†’T (Tâ‰ˆ10):

DÃ¹ng 
ğ›¼
(
ğ‘¡
)
Î±
(t)
 suy ra quyáº¿t Ä‘á»‹nh accept/reject theo trÃªn.

Cáº­p nháº­t 
ğ›¼
ğ‘˜
(
ğ‘¡
+
1
)
â†
ğ¾
â‹…
Pr
â¡
cal
(
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
Î±
k
(t+1)
	â€‹

â†Kâ‹…Pr
cal
	â€‹

(r
Î¸
	â€‹

(x)=0,yâˆˆG
k
	â€‹

).

Tráº£ 
ğ›¼
^
Î±
^
 há»™i tá»¥.

Chá»n 
ğœ‡
Î¼:

Vá»›i 
ğ¾
=
2
K=2 (head/tail), Ä‘áº·t 
ğœ‡
=
[
ğœ†
,
âˆ’
ğœ†
]
Î¼=[Î»,âˆ’Î»]. QuÃ©t 
ğœ†
âˆˆ
Î›
Î»âˆˆÎ› (grid nhá», vÃ­ dá»¥ 
[
âˆ’
2
,
2
]
[âˆ’2,2] step 0.1), má»—i 
ğœ†
Î» cháº¡y cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
Î±, chá»n 
ğœ†
^
Î»
^
 tá»‘i thiá»ƒu balanced selective error (hoáº·c AURC) trÃªn cal.

Vá»›i 
ğ¾
>
2
K>2: rÃ ng buá»™c 
âˆ‘
ğ‘˜
ğœ‡
ğ‘˜
=
0
âˆ‘
k
	â€‹

Î¼
k
	â€‹

=0 vÃ  dÃ¹ng EG nhá» 5â€“10 bÆ°á»›c Ä‘á»ƒ giáº£m balanced error (giá»¯ 
ğ›¼
Î± cáº­p nháº­t ná»™i vÃ²ng).

LÆ°u 
(
ğ›¼
^
,
ğœ‡
^
)
(
Î±
^
,
Î¼
^
	â€‹

) Ä‘á»ƒ dÃ¹ng á»Ÿ val/test.

7) Huáº¥n luyá»‡n gating báº±ng cáº­n PAC-Bayes

Ta cáº§n dataset S (Ä‘á»™c láº­p cal) Ä‘á»ƒ tá»‘i Æ°u bound (trÃ¡nh leak). Loss tuyáº¿n tÃ­nh cÃ¢n báº±ng (trÃªn S):

â„“
ğ›¼
^
(
ğœƒ
;
(
ğ‘¥
,
ğ‘¦
)
)
=
âˆ‘
ğ‘˜
=
1
ğ¾
1
ğ›¼
^
ğ‘˜
â€‰
1
[
ğ‘¦
âˆˆ
ğº
ğ‘˜
]
â‹…
1
[
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
0
]
â‹…
1
[
â„
ğœƒ
(
ğ‘¥
)
â‰ 
ğ‘¦
]
+
ğ‘
â€‰
1
[
ğ‘Ÿ
ğœƒ
(
ğ‘¥
)
=
1
]
.
â„“
Î±
^
	â€‹

(Î¸;(x,y))=
k=1
âˆ‘
K
	â€‹

Î±
^
k
	â€‹

1
	â€‹

1[yâˆˆG
k
	â€‹

]â‹…1[r
Î¸
	â€‹

(x)=0]â‹…1[h
Î¸
	â€‹

(x)
î€ 
=y]+c1[r
Î¸
	â€‹

(x)=1].

Chuáº©n hoÃ¡ 
â„“
~
âˆˆ
[
0
,
1
]
â„“
~
âˆˆ[0,1] báº±ng 
ğ¿
ğ›¼
^
=
max
â¡
{
ğ‘
,
âˆ‘
ğ‘˜
1
ğ›¼
^
ğ‘˜
}
+
ğ‘
L
Î±
^
	â€‹

=max{c,âˆ‘
k
	â€‹

Î±
^
k
	â€‹

1
	â€‹

}+c.
Bound cáº§n tá»‘i thiá»ƒu (Deterministic hoáº·c Stochastic 
ğ‘„
Q):

ğ¸
ğœƒ
âˆ¼
ğ‘„
â€‰â£
[
1
âˆ£
ğ‘†
âˆ£
âˆ‘
(
ğ‘¥
,
ğ‘¦
)
âˆˆ
ğ‘†
â„“
ğ›¼
^
(
ğœƒ
;
ğ‘¥
,
ğ‘¦
)
]
âŸ
empirical
+
â€…â€Š
ğ¿
ğ›¼
^
K
L
(
ğ‘„
âˆ¥
Î 
)
+
ln
â¡
â€‰â£
2
âˆ£
ğ‘†
âˆ£
ğ›¿
2
(
âˆ£
ğ‘†
âˆ£
âˆ’
1
)
â€…â€Š
+
â€…â€Š
ğœ†
â€‰
Î©
fair
(
ğ‘„
)
.
empirical
E
Î¸âˆ¼Q
	â€‹

	â€‹

âˆ£Sâˆ£
1
	â€‹

(x,y)âˆˆS
âˆ‘
	â€‹

â„“
Î±
^
	â€‹

(Î¸;x,y)
	â€‹

	â€‹

	â€‹

+L
Î±
^
	â€‹

2(âˆ£Sâˆ£âˆ’1)
KL(Qâˆ¥Î )+ln
Î´
2
âˆ£Sâˆ£
	â€‹

	â€‹

	â€‹

	â€‹

+Î»Î©
fair
	â€‹

(Q).

Hai má»©c triá»ƒn khai:

Thá»±c dá»¥ng (deterministic 
ğ‘„
=
ğ›¿
ğœƒ
Q=Î´
Î¸
	â€‹

): tá»‘i thiá»ƒu 
1
âˆ£
ğ‘†
âˆ£
âˆ‘
â„“
ğ›¼
^
(
ğœƒ
)
+
ğ›¾
âˆ¥
ğœƒ
âˆ¥
2
2
âˆ£Sâˆ£
1
	â€‹

âˆ‘â„“
Î±
^
	â€‹

(Î¸)+Î³âˆ¥Î¸âˆ¥
2
2
	â€‹

 (coi 
K
L
KL â‰ˆ L2). Nhanh, á»•n Ä‘á»‹nh, dá»… viáº¿t code.

Chuáº©n PAC-Bayes (Gaussian 
ğ‘„
Q): 
ğ‘„
=
ğ‘
(
ğœ‡
ğœƒ
,
ğœ
2
ğ¼
)
Q=N(Î¼
Î¸
	â€‹

,Ïƒ
2
I), 
Î 
=
ğ‘
(
0
,
ğœ
0
2
ğ¼
)
Î =N(0,Ïƒ
0
2
	â€‹

I). DÃ¹ng reparameterization trick Ä‘á»ƒ Æ°á»›c lÆ°á»£ng 
ğ¸
ğ‘„
[
â„“
]
E
Q
	â€‹

[â„“]; thÃªm term 
K
L
(
ğ‘„
âˆ¥
Î 
)
=
1
2
âˆ‘
ğ‘–
(
ğœ
2
+
ğœ‡
ğ‘–
2
ğœ
0
2
âˆ’
1
âˆ’
log
â¡
ğœ
2
ğœ
0
2
)
KL(Qâˆ¥Î )=
2
1
	â€‹

âˆ‘
i
	â€‹

(
Ïƒ
0
2
	â€‹

Ïƒ
2
+Î¼
i
2
	â€‹

	â€‹

âˆ’1âˆ’log
Ïƒ
0
2
	â€‹

Ïƒ
2
	â€‹

).

Group-aware: tÄƒng 
ğœ
0
Ïƒ
0
	â€‹

 (prior variance) cho tail-features hoáº·c thÃªm rÃ ng buá»™c budget KL theo group (thá»±c thi báº±ng pháº¡t 
âˆ‘
ğ‘”
ğœ‰
ğ‘”
â€‰
K
L
ğ‘”
âˆ‘
g
	â€‹

Î¾
g
	â€‹

KL
g
	â€‹

).

Huáº¥n luyá»‡n báº±ng Adam (lr 1e-3), batch 1024 entries (Ä‘á»c cache 
ğ‘
(
ğ‘š
)
(
ğ‘¥
)
p
(m)
(x)), 10â€“30 epochs; early-stop theo balanced selective error trÃªn val.

8) Má»Ÿ rá»™ng Worst-Group (Ïˆ = max)

VÃ²ng ngoÃ i: trá»ng sá»‘ nhÃ³m 
ğ›½
âˆˆ
Î”
ğ¾
âˆ’
1
Î²âˆˆÎ”
Kâˆ’1
 (khá»Ÿi uniform). Láº·p T=20â€“30:

Trong: thay 
1
ğ›¼
^
ğ‘˜
Î±
^
k
	â€‹

1
	â€‹

 báº±ng 
ğ›½
ğ‘˜
â‹…
1
ğ›¼
^
ğ‘˜
Î²
k
	â€‹

â‹…
Î±
^
k
	â€‹

1
	â€‹

 trong 
â„“
â„“, huáº¥n luyá»‡n gating nhÆ° Â§7 (ngáº¯n, 3â€“5 epochs fine-tune).

TÃ­nh lá»—i nhÃ³m 
ğ‘’
^
ğ‘˜
e
^
k
	â€‹

 trÃªn val; cáº­p nháº­t EG: 
ğ›½
ğ‘˜
âˆ
ğ›½
ğ‘˜
exp
â¡
(
ğœ‚
â€‰
ğ‘’
^
ğ‘˜
)
Î²
k
	â€‹

âˆÎ²
k
	â€‹

exp(Î·
e
^
k
	â€‹

); chuáº©n hoÃ¡ 
âˆ‘
ğ›½
ğ‘˜
=
1
âˆ‘Î²
k
	â€‹

=1.

Giá»¯ mÃ´ hÃ¬nh/gating tá»‘t nháº¥t theo worst-group selective error.

9) Suy luáº­n trÃªn test

Vá»›i 
ğ‘
(
ğ‘š
)
p
(m)
 (Ä‘Ã£ calibrate), cháº¡y gating 
ğ‘¤
(
ğ‘¥
;
ğœƒ
)
w(x;Î¸) â†’ 
ğ‘
ğ‘„
ğœƒ
(
ğ‘¥
)
p
Q
Î¸
	â€‹

	â€‹

(x).
Ãp 
(
ğ›¼
^
,
ğœ‡
^
)
(
Î±
^
,
Î¼
^
	â€‹

), 
ğ‘
c, quyáº¿t Ä‘á»‹nh báº±ng Theorem-1 plug-in â†’ nhÃ£n hoáº·c reject.
Náº¿u lÃ m worst-group: dÃ¹ng 
ğ›½
Î² cuá»‘i cÃ¹ng (hoáº·c há»—n há»£p iterate theo no-regret).

10) Metrics & cÃ¡ch tÃ­nh

Giáº£ sá»­ trÃªn táº­p 
ğ‘‡
T cÃ³ nhÃ³m 
ğº
ğ‘˜
G
k
	â€‹

, 
ğ‘˜
=
1..
ğ¾
k=1..K. KÃ½ hiá»‡u:

ğ‘Ÿ
(
ğ‘¥
)
âˆˆ
{
0
,
1
}
r(x)âˆˆ{0,1} (1=reject), 
ğ‘¦
^
(
ğ‘¥
)
y
^
	â€‹

(x) dá»± Ä‘oÃ¡n khi 
ğ‘Ÿ
=
0
r=0.

Coverage: 
c
o
v
=
1
âˆ’
Pr
â¡
ğ‘‡
(
ğ‘Ÿ
=
1
)
cov=1âˆ’Pr
T
	â€‹

(r=1).
Balanced selective error:

B
S
E
=
1
ğ¾
âˆ‘
ğ‘˜
=
1
ğ¾
Pr
â¡
ğ‘‡
(
ğ‘¦
^
â‰ 
ğ‘¦
âˆ£
ğ‘Ÿ
=
0
,
Â 
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
.
BSE=
K
1
	â€‹

k=1
âˆ‘
K
	â€‹

T
Pr
	â€‹

(
y
^
	â€‹

î€ 
=yâˆ£r=0,Â yâˆˆG
k
	â€‹

).

Worst-group selective error: 
max
â¡
ğ‘˜
Pr
â¡
ğ‘‡
(
ğ‘¦
^
â‰ 
ğ‘¦
âˆ£
ğ‘Ÿ
=
0
,
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
max
k
	â€‹

Pr
T
	â€‹

(
y
^
	â€‹

î€ 
=yâˆ£r=0,yâˆˆG
k
	â€‹

).
AURC: quÃ©t ngÆ°á»¡ng (hoáº·c â€œÄ‘Ã²n báº©yâ€ 
ğœ‡
Î¼/
ğ‘
c) Ä‘á»ƒ váº½ Riskâ€“Coverage (Risk = lá»—i cÃ³ Ä‘iá»u kiá»‡n trÃªn pháº§n Ä‘Æ°á»£c cháº¥p nháº­n), tÃ­nh diá»‡n tÃ­ch (trapezoid).
Accept-rate@group: 
Pr
â¡
ğ‘‡
(
ğ‘Ÿ
=
0
âˆ£
ğ‘¦
âˆˆ
ğº
ğ‘˜
)
Pr
T
	â€‹

(r=0âˆ£yâˆˆG
k
	â€‹

).
ECE@group: ECE tÃ­nh riÃªng cho má»—i nhÃ³m (bining theo confidence 
max
â¡
ğ‘¦
ğ‘
ğ‘„
ğœƒ
,
ğ‘¦
max
y
	â€‹

p
Q
Î¸
	â€‹

,y
	â€‹

 trÃªn pháº§n Ä‘Æ°á»£c cháº¥p nháº­n).

Khi bÃ¡o cÃ¡o, cá»‘ Ä‘á»‹nh coverage (vÃ­ dá»¥ 70%, 80%) vÃ  so sÃ¡nh BSE/WGSE táº¡i coverage Ä‘Ã³; kÃ¨m full RC-curve & AURC.

11) Ablation báº¯t buá»™c (viáº¿t paper)

Deterministic vs Gaussian 
ğ‘„
Q; prior thÆ°á»ng vs group-aware.

CÃ³/khÃ´ng calibration theo nhÃ³m.

Sá»‘ mÃ´ hÃ¬nh M & Ä‘a dáº¡ng (loss/sampling/augment).

So sÃ¡nh ghÃ©p: avg logits, log-opinion pool, PB-GSE, vÃ  PPRA (robust polytope) nhÆ° biáº¿n thá»ƒ.

áº¢nh hÆ°á»Ÿng 
ğ‘
c (0.05â€“0.5) vÃ  
ğœ‡
Î¼/
ğ›¼
Î± (grid size, vÃ²ng cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm).

Worst-group EG on/off.

Head vs Tail: báº£ng Accept-rate@group, ECE@group, lá»—i@coverageâ€”chá»©ng minh fair-ness cáº£i thiá»‡n.

12) Tá»• chá»©c code & tÃ¡i láº­p
pb-gse/
  configs/
    data.yaml        # Ä‘Æ°á»ng dáº«n, IF, seed, split tá»‰ lá»‡
    base_*.yaml      # cáº¥u hÃ¬nh tá»«ng base model
    gating.yaml      # MLP, features, prior, KL weight, Î´, ...
    plug_in.yaml     # c, grid Î», T_fixedpoint, K groups
  data/
    cifar_lt_indices.json
  models/
    backbones.py
    losses_lt.py     # LDAM-DRW, CB-Focal, BalancedSoftmax, ...
    calibration.py   # temp per-group, vector scaling
    gating.py        # MLP + features + (Gaussian Q) reparam
    plugin_rule.py   # Theorem-1 plug-in + fixed-point Î±, Î¼
  scripts/
    train_base.py
    dump_probs.py    # lÆ°u p^{(m)}(x) cho cal/val/test
    calibrate.py
    train_gating_pacbayes.py
    infer_plugin.py
    eval_metrics.py  # BSE/WGSE/AURC/ECE@group
  outputs/
    probs/{model}/cal|val|test.npz
    gating/{run_id}/theta.pt
    plugin/{run_id}/alpha_mu.json
    metrics/{run_id}/*.json


Seed & log: cá»‘ Ä‘á»‹nh seed (torch/cuda/python), log Hydra/Weights&Biases, lÆ°u hash git, thÃ´ng sá»‘ 
ğ‘
c, 
ğœ†
Î», 
ğ›¿
Î´, KL-weight.

13) Pseudocode (cÃ¡c bÆ°á»›c cá»‘t lÃµi)

(A) Cá»‘ Ä‘á»‹nh-Ä‘iá»ƒm 
ğ›¼
,
ğœ‡
Î±,Î¼ (K=2)

def fixed_point_alpha_mu(PQ_cal, y_cal, group_cal, c, lambdas, T=10):
    # PQ_cal: [N, C] probs tá»« p_Q
    best = None
    for lam in lambdas:       # mu_head=lam, mu_tail=-lam
        mu = { 'head': lam, 'tail': -lam }
        alpha = {'head': 1.0, 'tail': 1.0}
        for _ in range(T):
            # compute decisions using current alpha, mu
            acc_mask = []   # r(x)=0
            err_mask = []   # r=0 & wrong
            grp_mask = {'head': [], 'tail': []}
            for i,(p, y, g) in enumerate(zip(PQ_cal, y_cal, group_cal)):
                lhs = (p / alpha[g_idx(y, g)]).max()
                rhs = ((1/alpha['head'] - mu['head']) * p[head_ids].sum()
                     + (1/alpha['tail'] - mu['tail']) * p[tail_ids].sum() - c)
                accept = lhs >= rhs
                acc_mask.append(accept)
                grp_mask[g].append((i, accept))
                if accept and (y != argmax_balanced(p, alpha)): err_mask.append(i)
            # update alpha
            for g in ['head','tail']:
                accept_in_g = sum(1 for i,acc in grp_mask[g] if acc)
                total_in_g   = sum(1 for i,_ in grp_mask[g])
                alpha[g] = 2.0 * (accept_in_g / max(1, total_in_g))
        # compute balanced selective error on cal
        bse = 0.5 * sum( err_rate_in_group(...) for g in ['head','tail'] )
        if (best is None) or (bse < best['bse']):
            best = {'alpha': alpha.copy(), 'mu': mu.copy(), 'bse': bse, 'lambda': lam}
    return best


(B) Loss tuyáº¿n tÃ­nh cÃ¢n báº±ng cho gating

def balanced_linear_loss(PQ, y, g, alpha, mu, c):
    # One sample (vectorized trong code tháº­t)
    lhs = (PQ / alpha[g_idx_y_vector]).max()  # theo lá»›p
    rhs = ((1/alpha_grp - mu_grp) @ sum_probs_by_group(PQ)) - c
    accept = lhs >= rhs
    pred = argmax_balanced(PQ, alpha)
    err = (pred != y)
    return ((1/alpha[g]) * int(accept and err)) + c * int(not accept)


(C) Huáº¥n luyá»‡n gating theo bound (deterministic)

for epoch in range(E):
    for batch in loader_S:
        feats = build_features(P_list_batch, groups_batch)
        w = gating(feats)                     # softmax over M
        PQ = sum_m( w[:,m].unsqueeze(-1) * P_m_batch[m] )
        loss_emp = mean( balanced_linear_loss(PQ, y, g, alpha_hat, mu_hat, c) )
        loss = loss_emp + gamma * l2(theta) + lambda_fair * fairness_penalty(...)
        loss.backward(); opt.step(); opt.zero_grad()
    # early stop theo BSE trÃªn val

14) BÃ¡o cÃ¡o & hÃ¬nh váº½ (Ä‘Æ°a vÃ o paper)

Báº£ng chÃ­nh: BSE@{70,80,90% coverage}, WGSE@{70,80%}, AURC (â†“ tá»‘t), Accept-rate@group (â‰ˆ cÃ¢n), ECE@group (â†“).

HÃ¬nh: RC-curves theo nhÃ³m; histogram confidence trÆ°á»›c/sau calibration; map rejection trÃªn simplex (CIFAR-10).

Ablation: (i) +calibration; (ii) +PAC-Bayes (vs L2); (iii) +worst-group EG; (iv) #models; (v) thay PB-GSE báº±ng PPRA/Avg.

Checklist â€œÄ‘á»ƒ ná»™p paperâ€

 MÃ£ cháº¡y end-to-end vá»›i config cá»‘ Ä‘á»‹nh, 3 seeds, std-error.

 Tiá»n xá»­ lÃ½ rÃµ rÃ ng, LT-indices public.

 LÆ°u artifact: 
ğ‘
(
ğ‘š
)
p
(m)
, 
ğ›¼
^
,
ğœ‡
^
Î±
^
,
Î¼
^
	â€‹

, 
ğœƒ
Î¸, logs.

 BÃ¡o cÃ¡o Ä‘á»§ metric & hÃ¬nh; so sÃ¡nh baselines máº¡nh (paper gá»‘c, DeepEns+Chow, Conformal).