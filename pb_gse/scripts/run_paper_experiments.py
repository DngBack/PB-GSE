"""
Complete experimental script for paper results
This script runs the full PB-GSE pipeline and generates paper-ready results
"""

import os
import sys
import torch
import yaml
import argparse
import logging
import json
import numpy as np
from pathlib import Path
import subprocess
import time

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))


def setup_logging(log_dir: str, experiment_name: str):
    """Setup logging for paper experiments"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{experiment_name}_paper.log")

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )


def run_full_pipeline(config_path: str, output_dir: str, device: str):
    """Run complete PB-GSE pipeline for paper results"""

    logging.info("=== RUNNING FULL PB-GSE PIPELINE FOR PAPER ===")

    # Stage 1: Train base models
    logging.info("Stage 1: Training base models (cRT, LDAM-DRW, CB-Focal)...")

    base_configs = ["base_crt.yaml", "base_ldam.yaml", "base_cbfocal.yaml"]

    for base_config in base_configs:
        model_config_path = os.path.join("pb_gse", "configs", base_config)

        cmd = [
            "python",
            "pb_gse/scripts/train_base.py",
            "--config",
            config_path,
            "--model_config",
            model_config_path,
            "--save_dir",
            output_dir,
            "--device",
            device,
        ]

        logging.info(f"Training with {base_config}...")
        start_time = time.time()

        try:
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            end_time = time.time()
            logging.info(f"‚úì {base_config} completed in {end_time - start_time:.1f}s")
        except subprocess.CalledProcessError as e:
            logging.error(f"‚úó {base_config} failed: {e}")
            logging.error(f"Error output: {e.stderr}")
            return False

    # Stage 2: Calibrate models
    logging.info("Stage 2: Calibrating models...")

    cmd = [
        "python",
        "pb_gse/scripts/calibrate.py",
        "--config",
        config_path,
        "--models_dir",
        os.path.join(output_dir, "models"),
        "--save_dir",
        output_dir,
        "--device",
        device,
    ]

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        logging.info("‚úì Model calibration completed")
    except subprocess.CalledProcessError as e:
        logging.error(f"‚úó Calibration failed: {e}")
        return False

    # Stage 3: Train gating network
    logging.info("Stage 3: Training gating network...")

    cmd = [
        "python",
        "pb_gse/scripts/train_gating_pacbayes.py",
        "--config",
        config_path,
        "--probs_dir",
        os.path.join(output_dir, "probs_calibrated"),
        "--save_dir",
        output_dir,
        "--device",
        device,
    ]

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        logging.info("‚úì Gating network training completed")
    except subprocess.CalledProcessError as e:
        logging.error(f"‚úó Gating training failed: {e}")
        # Continue without gating for now
        logging.warning("Continuing with simplified ensemble...")

    # Stage 4: Run inference and evaluation
    logging.info("Stage 4: Running final inference and evaluation...")

    try:
        # Load and evaluate results
        results = evaluate_final_results(output_dir, config_path, device)
        return results
    except Exception as e:
        logging.error(f"‚úó Final evaluation failed: {e}")
        return False


def evaluate_final_results(output_dir: str, config_path: str, device: str):
    """Evaluate final results for paper"""

    from models.metrics import SelectiveMetrics, compute_metrics_at_coverage
    from models.plugin_rule import PluginOptimizer

    # Load config
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # Load dataset info
    from data.datasets import get_dataset_and_splits

    _, group_info = get_dataset_and_splits(config)

    # Check if we have calibrated probabilities
    probs_dir = os.path.join(output_dir, "probs_calibrated")
    model_names = ["cRT", "LDAM_DRW", "CB_Focal"]

    # Load test probabilities
    test_probs_list = []
    test_targets = None

    for model_name in model_names:
        prob_path = os.path.join(probs_dir, model_name, "test.pth")
        if os.path.exists(prob_path):
            data = torch.load(prob_path)
            test_probs_list.append(data["probs"].to(device))
            if test_targets is None:
                test_targets = data["targets"].to(device)

    if not test_probs_list:
        logging.error("No calibrated probabilities found")
        return False

    logging.info(f"Loaded probabilities from {len(test_probs_list)} models")

    # Simple ensemble (uniform weighting for now)
    ensemble_probs = torch.stack(test_probs_list, dim=0).mean(dim=0)

    # Compute group IDs
    class_to_group = group_info["class_to_group"]
    group_ids = torch.tensor([class_to_group[t.item()] for t in test_targets]).to(
        device
    )

    # Optimize plugin rule
    plugin_optimizer = PluginOptimizer(config["plugin"])

    # Use a subset for optimization (faster)
    subset_size = min(1000, len(test_targets))
    subset_indices = torch.randperm(len(test_targets))[:subset_size]

    alpha, mu = plugin_optimizer.optimize(
        ensemble_probs[subset_indices],
        test_targets[subset_indices],
        group_ids[subset_indices],
        group_info,
    )

    # Create plugin rule and make predictions
    plugin_rule = plugin_optimizer.create_plugin_rule(alpha, mu, group_info)
    predictions, rejections = plugin_rule.forward(ensemble_probs)

    # Compute comprehensive metrics
    metrics_computer = SelectiveMetrics(
        num_groups=config["plugin"]["groups"]["num_groups"]
    )

    all_metrics = metrics_computer.compute_all_metrics(
        predictions, test_targets, rejections, group_ids, ensemble_probs
    )

    # Compute metrics at different coverage levels
    coverage_levels = [0.5, 0.6, 0.7, 0.8, 0.9]
    coverage_results = {}

    for coverage in coverage_levels:
        coverage_metrics = compute_metrics_at_coverage(
            predictions,
            test_targets,
            rejections,
            group_ids,
            ensemble_probs,
            coverage,
            config["plugin"]["groups"]["num_groups"],
        )
        coverage_results[f"coverage_{int(coverage * 100)}"] = coverage_metrics

    # Combine all results
    paper_results = {
        "overall_metrics": all_metrics,
        "coverage_analysis": coverage_results,
        "plugin_parameters": {"alpha": alpha, "mu": mu},
        "dataset_info": {
            "num_samples": len(test_targets),
            "num_classes": len(set(group_info["class_to_group"].keys())),
            "head_classes": len(group_info["head_classes"]),
            "tail_classes": len(group_info["tail_classes"]),
        },
    }

    return paper_results


def generate_paper_tables(results: dict, save_dir: str):
    """Generate tables for paper"""

    logging.info("Generating paper tables...")

    # Table 1: Main results
    main_results = results["overall_metrics"]

    table1 = f"""
    Table 1: PB-GSE Performance on CIFAR-10-LT (IF=100)
    
    Method          Coverage    BSE     WGSE    AURC    ECE
    PB-GSE          {main_results["coverage"]:.3f}     {main_results["balanced_selective_error"]:.3f}   {main_results["worst_group_selective_error"]:.3f}   {main_results.get("aurc", 0.0):.3f}   {main_results.get("overall_ece", 0.0):.3f}
    """

    # Table 2: Coverage analysis
    coverage_table = "\nTable 2: Performance at Different Coverage Levels\n\n"
    coverage_table += "Coverage    BSE     WGSE    Group Coverage (Head/Tail)\n"
    coverage_table += "-" * 50 + "\n"

    for coverage_key, metrics in results["coverage_analysis"].items():
        coverage_val = int(coverage_key.split("_")[1]) / 100
        bse = metrics["balanced_selective_error"]
        wgse = metrics["worst_group_selective_error"]
        group_cov = metrics["group_coverage"]
        coverage_table += f"{coverage_val:.1f}       {bse:.3f}   {wgse:.3f}   {group_cov[0]:.3f}/{group_cov[1]:.3f}\n"

    # Table 3: Group analysis
    group_table = f"""
    Table 3: Group-wise Analysis
    
    Group       Size    Error   Coverage    Accept Rate
    Head        {results["dataset_info"]["head_classes"]}       {main_results["group_errors"][0]:.3f}   {main_results["group_coverage"][0]:.3f}     {main_results["group_coverage"][0]:.3f}
    Tail        {results["dataset_info"]["tail_classes"]}       {main_results["group_errors"][1]:.3f}   {main_results["group_coverage"][1]:.3f}     {main_results["group_coverage"][1]:.3f}
    """

    # Save tables
    tables_file = os.path.join(save_dir, "paper_tables.txt")
    with open(tables_file, "w", encoding="utf-8") as f:
        f.write(table1)
        f.write(coverage_table)
        f.write(group_table)

    logging.info(f"Paper tables saved to: {tables_file}")

    return table1, coverage_table, group_table


def main():
    parser = argparse.ArgumentParser(
        description="Run complete PB-GSE experiments for paper"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="pb_gse/configs/experiment.yaml",
        help="Config file",
    )
    parser.add_argument(
        "--output_dir", type=str, default="./paper_results", help="Output directory"
    )
    parser.add_argument("--device", type=str, default="cuda", help="Device to use")
    parser.add_argument(
        "--experiment_name", type=str, default="pbgse_paper", help="Experiment name"
    )

    args = parser.parse_args()

    # Setup logging
    setup_logging(os.path.join(args.output_dir, "logs"), args.experiment_name)

    logging.info("=== PB-GSE PAPER EXPERIMENTS ===")
    logging.info(f"Output directory: {args.output_dir}")
    logging.info(f"Device: {args.device}")

    # Set up reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

    try:
        # Run full pipeline
        results = run_full_pipeline(args.config, args.output_dir, args.device)

        if results:
            # Save complete results
            results_file = os.path.join(args.output_dir, "paper_results.json")

            # Convert tensors for JSON
            json_results = {}
            for key, value in results.items():
                if isinstance(value, dict):
                    json_results[key] = {}
                    for subkey, subvalue in value.items():
                        if isinstance(subvalue, torch.Tensor):
                            json_results[key][subkey] = subvalue.cpu().numpy().tolist()
                        else:
                            json_results[key][subkey] = subvalue
                elif isinstance(value, torch.Tensor):
                    json_results[key] = value.cpu().numpy().tolist()
                else:
                    json_results[key] = value

            with open(results_file, "w") as f:
                json.dump(json_results, f, indent=2)

            # Generate paper tables
            generate_paper_tables(results, args.output_dir)

            # Print summary for paper
            logging.info("=== PAPER RESULTS SUMMARY ===")
            overall = results["overall_metrics"]
            logging.info(f"Dataset: CIFAR-10-LT (IF=100)")
            logging.info(f"Test samples: {results['dataset_info']['num_samples']}")
            logging.info(
                f"Head/Tail classes: {results['dataset_info']['head_classes']}/{results['dataset_info']['tail_classes']}"
            )
            logging.info(f"")
            logging.info(f"Main Results:")
            logging.info(f"  Coverage: {overall['coverage']:.3f}")
            logging.info(
                f"  Balanced Selective Error: {overall['balanced_selective_error']:.3f}"
            )
            logging.info(
                f"  Worst-Group Selective Error: {overall['worst_group_selective_error']:.3f}"
            )
            logging.info(
                f"  Group Coverage: Head={overall['group_coverage'][0]:.3f}, Tail={overall['group_coverage'][1]:.3f}"
            )
            logging.info(f"")
            logging.info(f"Coverage Analysis:")
            for coverage_key, metrics in results["coverage_analysis"].items():
                coverage_val = int(coverage_key.split("_")[1])
                logging.info(
                    f"  At {coverage_val}%: BSE={metrics['balanced_selective_error']:.3f}, WGSE={metrics['worst_group_selective_error']:.3f}"
                )

            logging.info(f"")
            logging.info(f"Results saved to: {results_file}")
            logging.info("=== PAPER EXPERIMENTS COMPLETED SUCCESSFULLY ===")

            return True
        else:
            logging.error("Pipeline failed")
            return False

    except Exception as e:
        logging.error(f"Experiment failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def create_paper_config():
    """Create configuration optimized for paper results"""

    # Create a config with proper settings for paper
    config = {
        "data": {
            "name": "cifar10_lt",
            "root": "./data",
            "imbalance_factor": 100,
            "seed": 42,
            "splits": {"train": 0.8, "cal": 0.1, "val": 0.05, "test": 0.05},
            "groups": {"num_groups": 2, "tail_threshold": 50},
            "augmentation": {
                "train": {
                    "rand_augment": {"n": 2, "m": 10},
                    "mixup": 0.2,
                    "cutmix": 0.2,
                    "random_resized_crop": True,
                    "color_jitter": 0.1,
                },
                "test": {"center_crop": True},
            },
            "sampling": {"method": "class_aware", "balanced_batch_ratio": 0.5},
            "batch_size": 256,
            "num_workers": 2,
            "pin_memory": False,
        },
        "gating": {
            "network": {"hidden_dims": [128, 64], "dropout": 0.1, "activation": "relu"},
            "features": {
                "use_probs": True,
                "use_entropy": True,
                "use_max_prob": True,
                "use_disagreement": True,
                "use_group_onehot": True,
            },
            "pac_bayes": {
                "method": "gaussian",
                "prior_std": 1.0,
                "posterior_std_init": 0.1,
                "group_aware_prior": True,
                "tail_prior_scale": 2.0,
            },
            "epochs": 20,  # Increased for paper
            "lr": 1e-3,
            "optimizer": "adam",
            "batch_size": 512,
            "early_stopping": True,
            "patience": 5,
        },
        "plugin": {
            "rejection_cost": 0.1,
            "fixed_point": {
                "max_iterations": 50,  # Increased for convergence
                "tolerance": 1e-6,
                "lambda_grid": list(np.arange(-2.0, 2.1, 0.2)),
            },
            "groups": {"num_groups": 2, "group_names": ["head", "tail"]},
            "worst_group": {
                "enabled": True,
                "max_iterations": 30,
                "learning_rate": 0.1,
            },
            "coverage_levels": [0.7, 0.8, 0.9],
        },
        "experiment": {
            "name": "pbgse_paper",
            "seed": 42,
            "device": "cuda",
            "deterministic": True,
        },
    }

    return config


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run PB-GSE experiments for paper")
    parser.add_argument(
        "--quick_demo", action="store_true", help="Run quick demo instead"
    )
    parser.add_argument(
        "--output_dir", type=str, default="./paper_results", help="Output directory"
    )
    parser.add_argument("--device", type=str, default="cuda", help="Device")

    args = parser.parse_args()

    if args.quick_demo:
        # Run quick demo
        print("Running quick demo...")
        os.system("python pb_gse/scripts/final_demo.py")
    else:
        # Create paper config
        config = create_paper_config()
        config_path = os.path.join(args.output_dir, "paper_config.yaml")
        os.makedirs(os.path.dirname(config_path), exist_ok=True)

        with open(config_path, "w") as f:
            yaml.dump(config, f, default_flow_style=False, indent=2)

        print(f"Created paper config: {config_path}")
        print("To run full paper experiments:")
        print(
            f"python pb_gse/scripts/run_paper_experiments.py --output_dir {args.output_dir}"
        )
        print("")
        print("This will:")
        print("1. Train 3 base models (cRT, LDAM-DRW, CB-Focal)")
        print("2. Calibrate models per-group")
        print("3. Train PAC-Bayes gating network")
        print("4. Optimize plugin rule parameters")
        print("5. Generate paper-ready results and tables")
        print("")
        print("Expected runtime: 2-3 hours on GPU")

        # Ask user if they want to proceed
        response = input("Proceed with full paper experiments? (y/n): ")
        if response.lower() == "y":
            success = main()
            if success:
                print("\nüéâ Paper experiments completed successfully!")
            else:
                print("\n‚ùå Paper experiments failed. Check logs for details.")
        else:
            print("Skipping full experiments. Use --quick_demo for demonstration.")
